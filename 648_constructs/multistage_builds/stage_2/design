''' 1) Generate a combined list of all constructs
    - Collect and combine all of the 8 builds that comprise the 648 into one construct csv '''

import os
import pandas as pd

def find_csv_filenames(path_to_dir, exclusion_words, prefix = "fp_library_build_", suffix=".csv"):
    ''' Returns file names with a specified prefix, suffix, and excluding certain words (split by '_')'''
    filenames = os.listdir(path_to_dir)
    return [filename for filename in filenames if filename.startswith(prefix) and filename.endswith(suffix) and not any(word in filename.split('_') for word in exclusion_words)]

def combine_csvs(root_dir, exclusion_words):
    all_dataframes = []
    for subdir, dirs, files in os.walk(root_dir):
        for file in find_csv_filenames(subdir, exclusion_words):
            file_path = os.path.join(subdir, file)
            df = pd.read_csv(file_path)
            all_dataframes.append(df)

    combined_df = pd.concat(all_dataframes, ignore_index=True, sort=False)

    combined_df.replace(to_replace='BASIC_SEVA_37_CmR-p15A_v1.0', value='BASIC_SEVA_36_CmR-p15A_v1.0', inplace=True)
    combined_df.replace(to_replace='BASIC_BFP_ORF_v1.0', value='BASIC_mTagBFP2_ORF_v1.0', inplace=True)
    combined_df.replace(to_replace='BASIC_RFP_ORF_v1.0', value='BASIC_mCherry_ORF_v1.0', inplace=True)

    file_path = os.path.dirname(os.path.realpath(__file__))
    new_file_name = os.path.join(file_path, '648_combined_output.csv')
    combined_df.to_csv(new_file_name, index=False)

# Replace 'path/to/your/directory' with the path to the directory containing your subfolders
path = '200120_scripts_nbs\\648_constructs\\fp_library_builds'
exclusion_words = ['clip', 'assembly']
# combine_csvs(path, exclusion_words)



''' 2) Generate stage 2 construct csv from intermediate US and DS builds
    - Match each side of the combined construct csv to the stage 1 intermediate constructs.
    - Create a new construct csv for stage 2 with the intermediate constructs'''

import pandas as pd

def read_and_process_csv(file_path):
    df = pd.read_csv(file_path, header=0).iloc[:, 4:]

    df.replace(to_replace='BASIC_SEVA_37_CmR-p15A_v1.0', value='BASIC_SEVA_36_CmR-p15A_v1.0', inplace=True)
    df.replace(to_replace='BASIC_BFP_ORF_v1.0', value='BASIC_mTagBFP2_ORF_v1.0', inplace=True)
    df.replace(to_replace='BASIC_RFP_ORF_v1.0', value='BASIC_mCherry_ORF_v1.0', inplace=True)
    
    return df

def compare_construct_dfs(df1, df2, side):
    # Define the subset of columns to compare

    if side == 'US':            # which side of the construct
        subset_columns = ['Part 2', 'Linker 3', 'Part 3']
    else: # side == DS
        subset_columns = ['Part 4', 'Linker 5', 'Part 5']

    df2.columns = subset_columns

    # Filter df1 to the subset of columns
    df1_subset = df1[subset_columns]
    # print(df1_subset.head())
    # print(df2.head(36))

    def find_matching_index(row):
        for index, df2_row in df2.iterrows():
            if all(row[col] == df2_row[col] for col in subset_columns):
                return side + '_' + str(index+1)
        return None  # or a placeholder like -1

    # Apply the function to each row in df1
    return df1.apply(find_matching_index, axis=1)


# File paths for the two CSV files
US_construct_path = "648_constructs\\multistage_builds\\210221_36_constructs.csv"
DS_construct_path = "648_constructs\\multistage_builds\\ds constructs\\ds_constructs.csv"
combined_constructs = "648_constructs\\multistage_builds\\stage_2\\648_combined_output.csv"

# Process the CSV files
US_df = read_and_process_csv(US_construct_path).iloc[:, :3]
DS_df = read_and_process_csv(DS_construct_path).iloc[:, :3]
constructs_df = read_and_process_csv(combined_constructs)

# find stage 2 parts
backbones = pd.read_csv(combined_constructs, header=0).iloc[:, 1:4]
US_parts = compare_construct_dfs(constructs_df, US_df, 'US')
linkers = constructs_df['Linker 4']
DS_parts = compare_construct_dfs(constructs_df, DS_df, 'DS')

# Stage 2 build design
new_constructs_df = pd.read_csv(combined_constructs, header=0)
new_constructs_df['Part 2'] = US_parts
new_constructs_df['Linker 3'] = linkers
new_constructs_df['Part 3'] = DS_parts

blank_col = new_constructs_df['Part 6'].values[:, None]         # replace other columns with blanks
new_constructs_df.iloc[:, 7] = blank_col
new_constructs_df.iloc[:, 8] = blank_col
new_constructs_df.iloc[:, 9] = blank_col
new_constructs_df.iloc[:, 10] = blank_col

# print(new_constructs_df)

# Save the result to a new CSV file
file_path = os.path.dirname(os.path.realpath(__file__))
output_file = os.path.join(file_path, 'stage2_constructs.csv')
new_constructs_df.to_csv(output_file, index=False)


''' 3) Write parts and linkers csv '''

# # Find linkers used
# linker_list = new_constructs_df.iloc[:, [1, 3, 5]].stack().unique()
# print(linker_list, len(linker_list))

parts_columns  = ['Part/linker', 'Well', 'Part concentration (ng/uL)']
linker_list = ['LMS-P', 'LMS-S', 'LMP-P', 'LMP-S', 'UTR1-S', 'UTR1-RBS1-P', 'UTR1-RBS2-P', 'UTR1-RBS3-P', 'UTR2-S', 'UTR2-RBS1-P', 'UTR2-RBS2-P', 'UTR2-RBS3-P', 'UTR3-S', 'UTR3-RBS1-P', 'UTR3-RBS2-P', 'UTR3-RBS3-P']
parts_list = ['US_' + str(n+1) for n in range(36)] + ['DS_' + str(n+1) for n in range(18)] + [''] * 6 + ['BASIC_SEVA_36_CmR-p15A_v1.0'] + [''] * 11 + linker_list[:4] + [''] * 8 + linker_list[4:]
letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']
well_list = [l+str(n+1) for l in letters for n in range(12)]

parts_df_dict = {
    parts_columns[0]: parts_list,
    parts_columns[1]: well_list,
    parts_columns[2]: ['']*96
}

parts_df = pd.DataFrame(parts_df_dict)
output_file = os.path.join(file_path, 'stage2_parts.csv')
parts_df.to_csv(output_file, index=False)

# # Flatten the DataFrames into a list of unique values for each DataFrame
# flatten_df1 = pd.unique(df1.values.ravel('K'))
# flatten_df2 = pd.unique(df2.values.ravel('K'))

# # Convert to sets and compare
# same_unique_elements = set(flatten_df1) == set(flatten_df2)
